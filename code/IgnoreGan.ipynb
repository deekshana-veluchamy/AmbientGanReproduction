{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IgnoreGan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCsPu4DnRhDu",
        "colab_type": "text"
      },
      "source": [
        "# **IgnoreGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QosbDu3MfwWg",
        "colab_type": "code",
        "outputId": "aefdd6b5-4637-472d-b970-07c75118e461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install torchbearer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchbearer in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=0.4 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbearer) (0.2.2.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4->torchbearer) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->torchbearer) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdDnkfideMgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import os, sys, time\n",
        "import torchbearer\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor\n",
        "from skimage import color, data, restoration\n",
        "from scipy.signal import convolve2d\n",
        "from PIL import Image\n",
        "import glob\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib.pyplot import imshow\n",
        "import itertools\n",
        "import pickle\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import random \n",
        "import torch.utils.data as data\n",
        "from astropy.convolution import Gaussian2DKernel\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torchvision.utils import save_image\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUzjTBYGeSgq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1c5a94c3-fae8-4aab-aa45-1dbf3ea554f2"
      },
      "source": [
        "# Directory for saving the images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jYZkwgmRqHk",
        "colab_type": "text"
      },
      "source": [
        "# **Import data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY10_4_WefEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "677b0a3b-dfd4-499a-c427-848d744e2966"
      },
      "source": [
        "# The dataset is created\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "\n",
        "mnist_data = datasets.MNIST(root='data', train=True,download = True, transform=transform)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 19934291.09it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 331621.70it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5186759.30it/s]                           \n",
            "8192it [00:00, 132259.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ImCoG7Verpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dataloader \n",
        "full_loader = data.DataLoader(mnist_data, batch_size=128,\n",
        "                                    num_workers = 4,\n",
        "                                    pin_memory= True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ktLBcWibec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_dir = '/content/gdrive/My Drive/p_0/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMxzp543icsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(samples_dir):\n",
        "        os.mkdir(samples_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6MxdyiWIi0R",
        "colab_type": "text"
      },
      "source": [
        "# **Construct the Generator code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUvgDz_me7UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelG(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.z_dim = 100\n",
        "        super(ModelG, self).__init__()\n",
        "        self.fc2 = nn.Linear(10, 1000)\n",
        "        self.fc = nn.Linear(self.z_dim+1000, 64*28*28)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.deconv1 = nn.ConvTranspose2d(64, 32, 5, 1, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.deconv2 = nn.ConvTranspose2d(32, 1, 5, 1, 2)\n",
        "    \n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "    def forward(self, x, labels):\n",
        "        batch_size = x.size(0)\n",
        "        y_ = self.fc2(labels)\n",
        "        y_ = F.relu(y_)\n",
        "        x = torch.cat([x, y_], 1)\n",
        "        x = self.fc(x)\n",
        "        x = x.view(batch_size, 64, 28, 28)\n",
        "        x = self.bn1(x) \n",
        "        x = F.relu(x)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC1s5EKXIyAg",
        "colab_type": "text"
      },
      "source": [
        "# **Construct the Discriminator code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3EOUdzIfeNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelD, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, 1, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, 1, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.fc1  = nn.Linear(64*28*28+1000, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1)\n",
        "        self.fc3 = nn.Linear(10, 1000)\n",
        "    \n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "           \n",
        "    def forward(self, x, labels):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, 1, 28,28)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(batch_size, 64*28*28)\n",
        "        y_ = self.fc3(labels)\n",
        "        y_ = F.relu(y_)\n",
        "        x = torch.cat([x, y_], 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLJNasoCJUfj",
        "colab_type": "text"
      },
      "source": [
        "# **Constructing BetterCNN model for inception score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbjbtI4YkXCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Definition\n",
        "class BetterCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BetterCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 30, (5, 5), padding=0)\n",
        "        self.conv2 = nn.Conv2d(30, 15, (3, 3), padding=0)\n",
        "        self.fc1 = nn.Linear(15 * 5**2, 128)\n",
        "        self.fc2 = nn.Linear(128, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, (2,2))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, (2,2))\n",
        "        out = F.dropout(out,0.2)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF00gE5qtavN",
        "colab_type": "text"
      },
      "source": [
        "# **Block pixel transformation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdIZ4nwLexWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_block_pixel(entry, p=0, show_img = False):\n",
        "    if (type(entry) != torch.Tensor):\n",
        "        entry = torch.tensor(entry).permute(2,0,1) \n",
        "    copy = entry.clone()\n",
        "\n",
        "    if len(entry.shape) == 2:\n",
        "        copy = copy.unsqueeze(dim = 0)\n",
        "    a, b, c = copy.size()\n",
        "    mask = np.zeros(shape = copy.shape)\n",
        "    \n",
        "    \n",
        "    mask = torch.tensor(np.expand_dims(np.random.rand(b,c) < p, 0).astype(float)).byte()\n",
        "    copy[mask] = 0\n",
        "\n",
        "    \n",
        "    #plt.imshow(transforms.ToPILImage()(copy))                \n",
        "    return copy, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLxI22s1e1ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(data):\n",
        "        # Remove the number of channels\n",
        "        unbind_tuple = torch.unbind(data, 1)\n",
        "        #convert tuple to tensor\n",
        "        for var in unbind_tuple:\n",
        "          unbind_tensor = var\n",
        "        # Remove the batch size and map it to transformation function\n",
        "        block_pixel =list(map(apply_block_pixel, torch.unbind(unbind_tensor, 0)))\n",
        "        # Stack the unbinded tensors\n",
        "        output = torch.stack(list(zip(*block_pixel))[0],0)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_aK2fonKvcU",
        "colab_type": "text"
      },
      "source": [
        "# **Function for inception score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSqp5U5ZkZBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "def inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n",
        "  \n",
        "    N = len(imgs)\n",
        "    #print(N)\n",
        "    #print(type(imgs))\n",
        "\n",
        "    assert batch_size > 0\n",
        "    assert N > batch_size\n",
        "\n",
        "    # Set up dtype\n",
        "    if cuda:\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
        "        dtype = torch.FloatTensor\n",
        "\n",
        "    # Set up dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
        "\n",
        "    inception_model = BetterCNN()\n",
        "    inception_model.load_state_dict(torch.load('/content/gdrive/My Drive/Colab Notebooks/bettercnn.weights'))\n",
        "    inception_model.eval();\n",
        "\n",
        "   \n",
        "    def get_pred(x):\n",
        "        if resize:\n",
        "            x = up(x)\n",
        "        x = inception_model(x)\n",
        "        return F.softmax(x).data.cpu().numpy()\n",
        "\n",
        "    # Get predictions\n",
        "    preds = np.zeros((N, 10))\n",
        "\n",
        "    \n",
        "    #print(imgs.size())\n",
        "    preds = get_pred(imgs)\n",
        "    # Now compute the mean kl-div\n",
        "    split_scores = []\n",
        "\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = []\n",
        "        for i in range(part.shape[0]):\n",
        "            pyx = part[i, :]\n",
        "            scores.append(entropy(pyx, py))\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "\n",
        "       \n",
        "    return np.mean(split_scores), np.std(split_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpjkUxc6SIg_",
        "colab_type": "text"
      },
      "source": [
        "# **Loss Function and Optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTWXUU1-faMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "train_epoch = 25\n",
        "INPUT_SIZE = 784\n",
        "SAMPLE_SIZE = 1000\n",
        "NUM_LABELS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UykkdgLqUew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize weights\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGJ0YV3ifltj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Discriminator\n",
        "model_d = ModelD()\n",
        "# Create the Generator\n",
        "model_g = ModelG()\n",
        "\n",
        "#Create BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "#Initialize the input and noise\n",
        "input = torch.FloatTensor(128, INPUT_SIZE)\n",
        "noise = torch.FloatTensor(128, (100))\n",
        "\n",
        "#Create the batch of latent vectors\n",
        "fixed_noise = torch.FloatTensor(SAMPLE_SIZE, 100).normal_(0,1)\n",
        "fixed_labels = torch.zeros(SAMPLE_SIZE, NUM_LABELS)\n",
        "\n",
        "\n",
        "for i in range(NUM_LABELS):\n",
        "  for j in range(SAMPLE_SIZE // NUM_LABELS):\n",
        "    fixed_labels[i*(SAMPLE_SIZE // NUM_LABELS) + j, i] = 1.0\n",
        "    \n",
        "label = torch.FloatTensor(128)\n",
        "one_hot_labels = torch.FloatTensor(128, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQuvNOS6foot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_d.weight_init(mean=0.0, std=0.05)\n",
        "model_d.cuda()\n",
        "#model_g.weight_init(mean=0.0, std=0.05)\n",
        "model_g.cuda()\n",
        "input, label = input.cuda(), label.cuda()\n",
        "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "fixed_labels = fixed_labels.cuda()\n",
        "\n",
        "#Initialize SGD optimizers for both model_d and model_g\n",
        "optim_d = optim.SGD(model_d.parameters(), lr=0.01)\n",
        "optim_g = optim.SGD(model_g.parameters(), lr=0.01)\n",
        "\n",
        "fixed_noise = Variable(fixed_noise.cuda())\n",
        "fixed_labels = Variable(fixed_labels.cuda())\n",
        "\n",
        "#Establish real and fake lables for training\n",
        "real_label = 1\n",
        "fake_label = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSZ5XOzHSW1d",
        "colab_type": "text"
      },
      "source": [
        "# **Training Discriminator and Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJFKyFpDfsWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List to keep track of the the loss \n",
        "d_per_epoch_loss = []\n",
        "g_per_epoch_loss = []\n",
        "\n",
        "#For each epochs\n",
        "for epoch_idx in range(25):\n",
        "  \n",
        "  epoch_start_time = time.time()\n",
        "  \n",
        "  model_d.train()\n",
        "  model_g.train()\n",
        "  # learning rate decay\n",
        "  if (epoch_idx+1) == 15:\n",
        "        optim_d.param_groups[0]['lr'] /= 10\n",
        "        optim_g.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "  if (epoch_idx+1) == 20:\n",
        "        optim_d.param_groups[0]['lr'] /= 10\n",
        "        optim_g.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "  d_loss = []\n",
        "  g_loss = []\n",
        "  #For each batch in the data_loader\n",
        "  for batch_idx, (train_data_x, train_labels_y) in enumerate(full_loader):\n",
        "    ## Update the model_d network\n",
        "    # Train with all tranformed data batch\n",
        "    batch_size = train_data_x.size(0)\n",
        "    # Obtain the transformed data\n",
        "    train_data_x = transform(train_data_x)\n",
        "    train_data_x = train_data_x.view(-1, INPUT_SIZE)\n",
        "   \n",
        "    train_data_x = train_data_x.cuda()\n",
        "    train_labels_y = train_labels_y.cuda()\n",
        "\n",
        "    input.resize_as_(train_data_x).copy_(train_data_x)\n",
        "    label.resize_(batch_size).fill_(real_label)\n",
        "    one_hot_labels.resize_(batch_size, NUM_LABELS).zero_()\n",
        "    one_hot_labels = one_hot_labels.cuda()\n",
        "    one_hot_labels.scatter_(1, train_labels_y.view(batch_size,1), 1)\n",
        "    inputv = Variable(input)\n",
        "    labelv = Variable(label)\n",
        "    # Forward pass through model_d\n",
        "    output = model_d(inputv, Variable(one_hot_labels))\n",
        "    optim_d.zero_grad()\n",
        "    # Calculate the loss on transformed data batch\n",
        "    errD_real = criterion(output, labelv)\n",
        "    # Backward pass through model_d\n",
        "    errD_real.backward()\n",
        "    realD_mean = output.data.cpu().mean()\n",
        "    \n",
        "    # Train with all generated batch\n",
        "    one_hot_labels.zero_()\n",
        "    rand_y = torch.from_numpy(np.random.randint(0, NUM_LABELS, size=(batch_size,1))).cuda()\n",
        "    one_hot_labels.scatter_(1, rand_y.view(batch_size,1), 1)\n",
        "    noise.resize_(batch_size, 100).normal_(0,1)\n",
        "    label.resize_(batch_size).fill_(fake_label)\n",
        "    noisev = Variable(noise)\n",
        "    labelv = Variable(label)\n",
        "    onehotv = Variable(one_hot_labels)\n",
        "    # Generate the fake image\n",
        "    g_out = model_g(noisev, onehotv)\n",
        "    # Classify the fake image\n",
        "    output = model_d(g_out, onehotv)\n",
        "    # Calculate the loss on generated data batch\n",
        "    errD_fake = criterion(output, labelv)\n",
        "    fakeD_mean = output.data.cpu().mean()\n",
        "    # Add the gradients from the real_transformed and fake batches\n",
        "    errD = errD_real + errD_fake\n",
        "    # Calculate the gradients\n",
        "    errD_fake.backward()\n",
        "    # Update model_d\n",
        "    optim_d.step()\n",
        "\n",
        "    ## Update the model_g network\n",
        "    noise.normal_(0,1)\n",
        "    one_hot_labels.zero_()\n",
        "    rand_y = torch.from_numpy(np.random.randint(0, NUM_LABELS, size=(batch_size,1))).cuda()\n",
        "    one_hot_labels.scatter_(1, rand_y.view(batch_size,1), 1)\n",
        "    label.resize_(batch_size).fill_(real_label)\n",
        "    onehotv = Variable(one_hot_labels)\n",
        "    noisev = Variable(noise)\n",
        "    labelv = Variable(label)\n",
        "    # Generate the fake image\n",
        "    g_out = model_g(noisev, onehotv)\n",
        "    output = model_d(g_out, onehotv)\n",
        "    errG = criterion(output, labelv)\n",
        "    optim_g.zero_grad()\n",
        "    errG.backward()\n",
        "    # Update model_g\n",
        "    optim_g.step()\n",
        "  \n",
        "  \n",
        "    d_loss.append(errD)\n",
        "    g_loss.append(errG)\n",
        "    \n",
        "  epoch_end_time = time.time()  \n",
        "  \n",
        "  # Save the generated image\n",
        "  g_out = model_g(fixed_noise, fixed_labels).data.view(SAMPLE_SIZE, 1, 28,28).cpu()\n",
        "  save_image(g_out,'{}/{}.png'.format(samples_dir, epoch_idx))\n",
        "  \n",
        " \n",
        "  # Append the losses\n",
        "  d_per_epoch_loss.append(torch.mean(torch.stack(d_loss),dim=0)) \n",
        "  g_per_epoch_loss.append(torch.mean(torch.stack(g_loss),dim=0))\n",
        "  \n",
        "  \n",
        "  # calculate inception score\n",
        "  if epoch_idx==24:\n",
        "    print('Inception score:',inception_score(g_out))\n",
        "  \n",
        "  \n",
        "  print('Epoch {} - D loss = {:.4f}, G loss = {:.4f}, Time taken in sec = {:.4f}'.format(epoch_idx, d_per_epoch_loss[epoch_idx], g_per_epoch_loss[epoch_idx],epoch_end_time-epoch_start_time))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLHF0OuYSnod",
        "colab_type": "text"
      },
      "source": [
        "# **Plotting the loss versus training iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlImlVvkZRCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss\")\n",
        "plt.plot(g_per_epoch_loss,label=\"G\")\n",
        "plt.plot(d_per_epoch_loss,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
